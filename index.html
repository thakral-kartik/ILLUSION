<!DOCTYPE html>

<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>ILLUSION: Integration of Life Like Unique Synthetic Identities and Objects from Neural Networks</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="ILLUSION: Integration of Life Like Unique Synthetic Identities and Objects from Neural Networks">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://https://bhashabluff.github.io/ILLUSION//ILLUSION/">
<meta property="og:url" content="https://https://bhashabluff.github.io/ILLUSION//ILLUSION//">
<meta name="twitter:card" content="summary">
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="asset/diffgan-tts/style.css">
  </head>
  <body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
    <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->
      
      
    </section>

    <section class="main-content">
      <h1 id=""><center> ILLUSION: Integration of Life Like Unique Synthetic Identities and Objects from Neural Networks </center></h1>

<center> Anonymous Authors </center>
<!-- <center> <sup>2</sup> Tencent AI Lab </center> -->


<h2 id="abstract">Abstract</h2>
<p>
The recent surge in the spread of deepfakes and AI-generated content on social media platforms has led to a significant increase in media forgeries, thereby intensifying the need for effective detection research. However, there is a conspicuous absence of a comprehensive dataset that encapsulates various types of fake media across all three modalities: image, audio, and video. This study addresses this gap by introducing <b>ILLUSION</b> (<b>I</b>ntegration of <b>L</b>ife-<b>L</b>ike <b>U</b>nique <b>S</b>ynthetic <b>I</b>dentities and Objects from <b>N</b>eural Networks), a large-scale, multi-modal deepfake dataset. This first-of-its-kind dataset is created using a variety of state-of-the-art generative models and includes face swaps, audio spoofs, audio-video synchronized and synthetically generated images, faces, audio, and videos. The dataset, balanced in terms of sex and skin tone, comprises a total of 1,371,986 samples. The ILLUSION dataset is designed to facilitate the development of deepfake detection systems that are unified across all three modalities and robust against all types of fake media. Furthermore, we benchmark the dataset with popular and state-of-the-art algorithms for images, audio, and videos, demonstrating its utility and effectiveness in the field of deepfake detection research.
</p>
<h2><p class="toc_title">Contents</p></h2>
<div id="toc_container">
<ul>
  <li><a href="https://https://bhashabluff.github.io/ILLUSION//#0">ILLUSION Dataset Design</a></li>
  <li><a href="https://https://bhashabluff.github.io/ILLUSION//#1">Comparison With Existing Dataset</a></li>
  <li><a href="https://https://bhashabluff.github.io/ILLUSION//#2">Dataset Statistics</a></li>
</ul>
</div>
<br>

<a name="0" href="https://https://bhashabluff.github.io/ILLUSION//"><h2 id="overview">Dataset Design</h2></a>
<div style="text-align: center; width: 1000px;"> 
<img alt="" src="viz_abs_ILLUSION.png" style="display: inline-block; vertical-align: middle;" />
</div>

<br>
<br>
<a name="1" href="https://bhashabluff.github.io/ILLUSION//"><h2 id="visualization1">Comparison With Existing Dataset</h2></a>
<div style="text-align: center; width: 600px;"> 
<img alt="" src="dataset_comp.png" style="display: inline-block; vertical-align: middle;" />
</div>

<br>
<br>

<a name="2" href="https://bhashabluff.github.io/ILLUSION//"><h2 id="visualization2">Dataset Statistics</h2></a>
<div style="text-align: center; width: 600px;"> 
<img alt="" src="results_illusion.png" style="display: inline-block; vertical-align: middle;" />
</div>

<br>
<br>
        
<div class="col-md-5"><h2>Leaderboard</h2><p> How your model perform in detecting deepfakes?</p><table class="table performanceTable"><tr><th>Rank</th><th>Date</th><th>Model</th><th>EER</th><th>AUC</th></tr><tr><td class="rank">1 <br></td><td><span class="date label label-default">Aug 17, 2024</span></td><td style="word-break:break-word;">AASIST</td><td><b>0.506</b></td><td>0.471</td></tr><tr><td class="rank">2 <br></td><td><span class="date label label-default">Aug 17, 2024</span></td><td style="word-break:break-word;">Conformer</td><td>0.523</td><td>0.488</td></tr>
<tr><td class="rank">3 <br></td><td><span class="date label label-default">Aug 17, 2024</span></td><td style="word-break:break-word;">RawGAT-ST</td><td>0.571</td><td>0.402</td></tr>
<tr><td class="rank">4 <br></td><td><span class="date label label-default">Aug 17, 2024</span></td><td style="word-break:break-word;">SSLModel</td><td>0.578</td><td>0.397</td></tr></table>

<h3>How can I participate?</h3><p>Update: the competition is open.</p></div></div></div>

<div class="container"><div class="row"><div class="col-md-8"><h2>How is the Mulitlingual test designed?</h2><p> The test set consists of 500 studies from 500 unseen patients. Eight board-certified radiologists individually annotated each of the studies in the test set, classifying each observation into one of present, uncertain likely, uncertain unlikely, and absent. Their annotations were binarized such that all present and uncertain likely cases are treated as positive and all absent and uncertain unlikely cases are treated as negative.  
The majority vote of 5 radiologist annotations serves as a strong ground truth; the remaining 3 radiologist annotations were used to benchmark radiologist performance.<p>For each of the 3 individual radiologists and for their majority vote, we compute sensitivity (recall), specificity, and precision against the test set ground truth. To compare the model to radiologists, we plot the radiologist operating points with the model on both the ROC and Precision-Recall (PR) space. We examine whether the radiologist operating points lie below the curves to determine if the model is superior to the radiologists.</p></p></div></div></div>

<div class="container"><div class="row"><div class="col-md-7"><h2>What is our baseline model?</h2><p>We train models that take as input a single-view chest radiograph and output the probability of each of the 14 observations. When more than one view is available, the models output the maximum probability of the observations across the views.</p></div></div></div>

</section>
</body></html>
