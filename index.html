<!DOCTYPE html>

<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>ILLUSION: Integration of Life Like Unique Synthetic Identities and Objects from Neural Networks</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="ILLUSION: Integration of Life Like Unique Synthetic Identities and Objects from Neural Networks">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://https://bhashabluff.github.io/ILLUSION//ILLUSION/">
<meta property="og:url" content="https://https://bhashabluff.github.io/ILLUSION//ILLUSION//">
<meta name="twitter:card" content="summary">
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="asset/diffgan-tts/style.css">
  </head>
  <body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
    <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->
      
      
    </section>

    <section class="main-content">
      <h1 id=""><center> ILLUSION: Integration of Life Like Unique Synthetic Identities and Objects from Neural Networks </center></h1>

<center> Anonymous Authors </center>
<!-- <center> <sup>2</sup> Tencent AI Lab </center> -->


<h2 id="abstract">Abstract</h2>
<p>
The recent surge in the spread of deepfakes and AI-generated content on social media platforms has led to a significant increase in media forgeries, thereby intensifying the need for effective detection research. However, there is a conspicuous absence of a comprehensive dataset that encapsulates various types of fake media across all three modalities: image, audio, and video. This study addresses this gap by introducing <b>ILLUSION</b> (<b>I</b>ntegration of <b>L</b>ife-<b>L</b>ike <b>U</b>nique <b>S</b>ynthetic <b>I</b>dentities and Objects from <b>N</b>eural Networks), a large-scale, multi-modal deepfake dataset. This first-of-its-kind dataset is created using a variety of state-of-the-art generative models and includes face swaps, audio spoofs, audio-video synchronized and synthetically generated images, faces, audio, and videos. The dataset, balanced in terms of sex and skin tone, comprises a total of 1,371,986 samples. The ILLUSION dataset is designed to facilitate the development of deepfake detection systems that are unified across all three modalities and robust against all types of fake media. Furthermore, we benchmark the dataset with popular and state-of-the-art algorithms for images, audio, and videos, demonstrating its utility and effectiveness in the field of deepfake detection research.
</p>
<h2><p class="toc_title">Contents</p></h2>
<div id="toc_container">
<ul>
  <li><a href="https://https://bhashabluff.github.io/ILLUSION//#0">ILLUSION Dataset Design</a></li>
  <li><a href="https://https://bhashabluff.github.io/ILLUSION//#1">Comparison With Existing Dataset</a></li>
  <li><a href="https://https://bhashabluff.github.io/ILLUSION//#2">Dataset Statistics</a></li>
</ul>
</div>
<br>

<a name="0" href="https://https://bhashabluff.github.io/ILLUSION//"><h2 id="overview">Dataset Design</h2></a>
<div style="text-align: center; width: 1000px;"> 
<img alt="" src="viz_abs_ILLUSION.png" style="display: inline-block; vertical-align: middle;" />
</div>

<br>
<br>
<a name="1" href="https://bhashabluff.github.io/ILLUSION//"><h2 id="visualization1">Comparison With Existing Dataset</h2></a>
<div style="text-align: center; width: 600px;"> 
<img alt="" src="dataset_comp.png" style="display: inline-block; vertical-align: middle;" />
</div>

<br>
<br>

<a name="2" href="https://bhashabluff.github.io/ILLUSION//"><h2 id="visualization2">Dataset Statistics</h2></a>
<div style="text-align: center; width: 600px;"> 
<img alt="" src="results_illusion.png" style="display: inline-block; vertical-align: middle;" />
</div>

<br>
<br>
        
<div class="col-md-5"><h2>Leaderboard</h2><p> How your model perform in detecting deepfakes?</p><table class="table performanceTable"><tr><th>Rank</th><th>Date</th><th>Model</th><th>EER</th><th>AUC</th></tr><tr><td class="rank">1 <br></td><td><span class="date label label-default">Aug 17, 2024</span></td><td style="word-break:break-word;">AASIST</td><td><b>0.506</b></td><td>0.471</td></tr><tr><td class="rank">2 <br></td><td><span class="date label label-default">Aug 17, 2024</span></td><td style="word-break:break-word;">Conformer</td><td>0.523</td><td>0.488</td></tr>
<tr><td class="rank">3 <br></td><td><span class="date label label-default">Aug 17, 2024</span></td><td style="word-break:break-word;">RawGAT-ST</td><td>0.571</td><td>0.402</td></tr>
<tr><td class="rank">4 <br></td><td><span class="date label label-default">Aug 17, 2024</span></td><td style="word-break:break-word;">SSLModel</td><td>0.578</td><td>0.397</td></tr></table></div>
        
<div class="container"><div class="row"><div class="col-md-8"><h2>How can I participate?</h2><p> The Competition is currently closed. We will accept submissions after acceptance of the paper. </p></p></div></div></div>

<div class="container"><div class="row"><div class="col-md-8"><h2>How is the Mulitlingual test designed?</h2><p> To enhance the dataset’s robustness and global relevance, we introduce Set D—a multi-lingual, multi-modal test set. Set D comprises two subsets: D.1 includes 4160 multi-lingual audio samples (2560 of which are spoofs), while D.2 contains 120 multi-lingual multi-modal deepfakes. These samples represent over 26 different languages from diverse geographical locations, including English, German, French, Russian, Hindi, Spanish, Japanese, Chinese, Sanskrit, Marathi, Odiya, Bhojpuri, Nepali, Sindhi, Irish, Romanian, Latin, Indonesian, Icelandic, Punjabi, Kannada, Gujarati, Korean, Telugu, Arabic, and Italian. Notably, Set D is curated from online sources, mimicking real-world zero-day attack scenarios, with unknown generation algorithms.

By incorporating Set D, we not only broaden the dataset’s applicability but also gain valuable insights into detection model performance across various linguistic contexts. Additionally, we acknowledge that Set B currently includes only English samples due to limitations in text-to-modality models. However, we anticipate extending this in the future as text-to-modality generative models advance.
</p></p></div></div></div>

<div class="container"><div class="row"><div class="col-md-7"><h2>What is our baseline model?</h2><p>We utilize four state-of-the-art video and four audio deepfake detection models to benchmark all three sets of the proposed dataset. For video deepfake detection, we employ MesoInceptionNet, XceptionNet, DSP-FWA, and F3Net. For audio deepfake detection, we use RawGAT-ST, AASIST, SSLModel, and Conformer. We also benchmark the proposed dataset using multimodal deepfake detection algorithms. Specifically, we employ state-of-the-art methods such as MRDF and FACTOR from the literature. Additionally, we use an ensemble of F3Net and SSLModel, which are baseline unimodal models (referred to as unimodal ensembling model), and report class-wise video-level accuracy.</p></div></div></div>

</section>
</body></html>
